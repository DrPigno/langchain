{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639559f5",
   "metadata": {},
   "source": [
    "# Lezione 2: Agenti Avanzati e Integrazioni\n",
    "\n",
    "Benvenuti alla seconda lezione! In questo notebook imparerete:\n",
    "\n",
    "1. üìù **Summarize Middleware**: Riassumere conversazioni lunghe automaticamente\n",
    "2. ü§ù **Human-in-the-Loop**: Intervento umano nelle decisioni dell'agente\n",
    "3. üóÑÔ∏è **SQL Agent**: Interrogare database con linguaggio naturale\n",
    "4. üìö **RAG**: Retrieval Augmented Generation con PDF e Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8aa36",
   "metadata": {},
   "source": [
    "## Setup Iniziale\n",
    "\n",
    "Verifichiamo l'ambiente e carichiamo le dipendenze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carica variabili d'ambiente\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Verifica API keys\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OpenAI API key trovata\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OpenAI API key non trovata - necessaria per utilizzare i modelli OpenAI\")\n",
    "    print(\"   Ottieni una chiave su https://platform.openai.com/account/api-keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab60f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza il modello\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"openai:gpt-4o-mini\",  # Usiamo un modello pi√π potente per questi task\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modello inizializzato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df110bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con Cerebras\n",
    "#from langchain_cerebras import ChatCerebras\n",
    "\n",
    "#model = ChatCerebras(model_name=\"gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b85884",
   "metadata": {},
   "source": [
    "## 1. üìù Summarize Middleware\n",
    "\n",
    "Per conversazioni lunghe, la **SummarizationMiddleware** riassume automaticamente i messaggi pi√π vecchi per risparmiare token e mantenere il contesto gestibile.\n",
    "\n",
    "**Quando usarlo:**\n",
    "- Conversazioni molto lunghe (> 20-30 messaggi)\n",
    "- Limiti di contesto del modello\n",
    "- Costi elevati per token\n",
    "\n",
    "- Token counter configurabile\n",
    "\n",
    "**Novit√† con la nuova API:**- Gestione automatica di coppie AI/Tool message\n",
    "\n",
    "- Middleware ufficiale da `langchain.agents.middleware.summarization`- Supporto per trigger multipli (token, messaggi, frazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di configurazione avanzata con trigger multipli\n",
    "from langchain.agents.middleware.summarization import SummarizationMiddleware\n",
    "\n",
    "# Trigger quando SI VERIFICA UNA delle condizioni:\n",
    "advanced_summarization = SummarizationMiddleware(\n",
    "    model=model,\n",
    "    trigger=[\n",
    "        (\"messages\", 50),   # O quando raggiungi 50 messaggi\n",
    "        (\"tokens\", 4000),   # O quando raggiungi 4000 token\n",
    "        #(\"fraction\", 0.8)   # O quando usi l'80% del contesto del modello se supportato\n",
    "    ],\n",
    "    keep=(\"tokens\", 2000),  # Mantieni gli ultimi 2000 token\n",
    "    # token_counter: funzione custom per contare i token (opzionale)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SummarizationMiddleware avanzato creato\")\n",
    "print(\"\\nüîÆ Modalit√† di trigger disponibili:\")\n",
    "print(\"   - ('messages', N): Numero di messaggi\")\n",
    "print(\"   - ('tokens', N): Numero di token assoluti\")\n",
    "print(\"   - ('fraction', F): Frazione del contesto del modello (0.0-1.0)\")\n",
    "print(\"\\nüîÆ Modalit√† di keep disponibili:\")\n",
    "print(\"   - ('messages', N): Mantieni N messaggi recenti\")\n",
    "print(\"   - ('tokens', N): Mantieni N token recenti\")\n",
    "print(\"   - ('fraction', F): Mantieni F frazione del contesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3781e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware.summarization import SummarizationMiddleware\n",
    "\n",
    "# Creiamo un checkpointer per la memoria\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Creiamo il middleware di summarization\n",
    "summarization_middleware = SummarizationMiddleware(\n",
    "    model=model,\n",
    "    # Trigger: quando la conversazione raggiunge 5 messaggi\n",
    "    trigger=(\"messages\", 5),\n",
    "    # Keep: mantieni gli ultimi 2 messaggi dopo il riassunto\n",
    "    keep=(\"messages\", 2)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Middleware di summarization creato\")\n",
    "print(f\"   - Keep: {summarization_middleware.keep}\")\n",
    "print(f\"   - Trigger: {summarization_middleware.trigger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un agente con summarization middleware\n",
    "agent_with_summary = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[summarization_middleware],\n",
    "    checkpointer=memory,\n",
    "    system_prompt=\"\"\"Sei un assistente che mantiene conversazioni lunghe.\n",
    "    \n",
    "Grazie al middleware di summarization, posso gestire conversazioni\n",
    "di centinaia di messaggi senza perdere il contesto o superare i\n",
    "limiti di token del modello.\n",
    "\n",
    "Rispondi sempre in italiano.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agente con SummarizationMiddleware creato!\")\n",
    "print(\"\\nüìö Comportamento:\")\n",
    "print(\"   1. Quando la conversazione raggiunge 5 messaggi\")\n",
    "print(\"   2. Il middleware crea automaticamente un riassunto dei primi 30\")\n",
    "print(\"   3. Mantiene solo gli ultimi 2 messaggi + il riassunto\")\n",
    "print(\"   4. Riduce i token e mantiene il contesto rilevante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c118050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage\n",
    "from uuid import uuid4\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid4())}}\n",
    "\n",
    "response = agent_with_summary.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Ciao!\"),\n",
    "        AIMessage(content=\"Ciao! Come posso aiutarti oggi?\"),\n",
    "        HumanMessage(content=\"Sai qual √® il senso della vita?\"),\n",
    "        AIMessage(content=\"Certo! Il senso della vita √® 42, ovviamente.\"),\n",
    "        HumanMessage(content=\"Cio√®? Puoi spiegarti meglio?\"),\n",
    "        AIMessage(content=\"Beh, √® una risposta filosofica tratta da 'Guida Galattica per Autostoppist'. Per√≤ io ci credo davvero.\"),\n",
    "        HumanMessage(content=\"Interessante! Non pensavo che le intelligenze artificiali potessero avere opinioni filosofiche.\"),\n",
    "    ]}, \n",
    "    config=config)\n",
    "\n",
    "print(\"ü§ñ Risposta dell'agente con summarization:\\n\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7181a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in response['messages']:\n",
    "    r.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f30dc1",
   "metadata": {},
   "source": [
    "## 2. ü§ù Human-in-the-Loop Middleware\n",
    "\n",
    "**Human-in-the-loop** permette all'agente di chiedere conferma prima di eseguire azioni sensibili.\n",
    "\n",
    "**Casi d'uso:**\n",
    "- Operazioni critiche (cancellazioni, pagamenti)\n",
    "- Decisioni ambigue\n",
    "- Approvazioni workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def delete_file(filename: str) -> str:\n",
    "    \"\"\"Elimina un file dal sistema. ATTENZIONE: operazione irreversibile!\n",
    "    \n",
    "    Args:\n",
    "        filename: Nome del file da eliminare\n",
    "    \"\"\"\n",
    "    # In un sistema reale, qui ci sarebbe la logica di eliminazione\n",
    "    return f\"‚ö†Ô∏è SIMULAZIONE: File '{filename}' sarebbe stato eliminato\"\n",
    "\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Invia una email.\n",
    "    \n",
    "    Args:\n",
    "        to: Destinatario\n",
    "        subject: Oggetto\n",
    "        body: Corpo del messaggio\n",
    "    \"\"\"\n",
    "    return f\"üìß SIMULAZIONE: Email inviata a {to}\\nOggetto: {subject}\"\n",
    "\n",
    "print(\"‚úÖ Tools sensibili creati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abe666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementazione Human-in-the-Loop con interrupt() di LangGraph\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "@tool\n",
    "def delete_file_with_approval(filename: str) -> str:\n",
    "    \"\"\"Elimina un file dal sistema con approvazione umana richiesta.\n",
    "    \n",
    "    Args:\n",
    "        filename: Nome del file da eliminare\n",
    "    \"\"\"\n",
    "    # Richiedi approvazione usando interrupt()\n",
    "    approval = interrupt(\n",
    "        {\n",
    "            \"action\": \"delete_file\",\n",
    "            \"filename\": filename,\n",
    "            \"message\": f\"‚ö†Ô∏è Vuoi davvero eliminare '{filename}'? Questa operazione √® irreversibile.\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if approval and approval.get(\"approved\"):\n",
    "        return f\"‚úÖ File '{filename}' eliminato con successo\"\n",
    "    else:\n",
    "        return f\"‚ùå Eliminazione di '{filename}' annullata\"\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def send_email_with_approval(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Invia una email con approvazione umana richiesta.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        to: Destinatario\n",
    "\n",
    "        subject: Oggetto        \n",
    "\n",
    "        body: Corpo del messaggio    \n",
    "\n",
    "    \"\"\"       \n",
    "    approval = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"message\": f\"üìß Vuoi inviare questa email a {to}?\",\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "    })\n",
    "        # Richiedi approvazione usando interrupt()  \n",
    "    if approval and approval.get(\"approved\"):\n",
    "        return f\"üìß Email inviata con successo a {to}\"\n",
    "    else:\n",
    "        return f\"‚ùå Invio email a {to} annullato\"\n",
    "\n",
    "print(\"- L'esecuzione dell'agente viene sospesa\")\n",
    "print(\"\\nüí° Con interrupt() di LangGraph:\")\n",
    "print(\"- L'applicazione pu√≤ chiedere conferma all'utente\")\n",
    "print(\"‚úÖ Tools con Human-in-the-Loop creati\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741491ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un agente con Human-in-the-Loop\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "hil_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[delete_file_with_approval, send_email_with_approval],\n",
    "    checkpointer=memory,  # Necessario per gestire gli interrupt\n",
    "    system_prompt=\"\"\"Sei un assistente che esegue operazioni sensibili.\n",
    "\n",
    "Quando l'utente ti chiede di eliminare un file o inviare un'email:\n",
    "1. USA IMMEDIATAMENTE il tool appropriato (delete_file_with_approval o send_email_with_approval)\n",
    "2. Il tool stesso gestir√† la richiesta di approvazione con interrupt()\n",
    "3. NON chiedere conferma con un messaggio - usa direttamente il tool\n",
    "\n",
    "Rispondi sempre in italiano.\"\"\"\n",
    "\n",
    ")\n",
    "print(\"   1. Esegui: \")\n",
    "config = {'configurable': {'thread_id': str(uuid4())}}\n",
    "print(\"   2. Prima invocazione: \")\n",
    "response = hil_agent.invoke({'messages': 'Elimina report_vecchio.pdf'}, config)\n",
    "print(\"   3. L'agente si ferma e restituisce un interrupt\")\n",
    "for r in response['messages']:\n",
    "    r.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"   4. Riprendi con: \")\n",
    "response = hil_agent.invoke(Command(resume={'approved': True}), config)\n",
    "\n",
    "print(f\"\\nüìä Numero totale di messaggi: {len(response['messages'])}\")\n",
    "print(f\"Tipi di messaggi: {[msg.type for msg in response['messages']]}\\n\")\n",
    "\n",
    "for r in response['messages']:\n",
    "    r.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ff7aa",
   "metadata": {},
   "source": [
    "## 3. üóÑÔ∏è SQL Agent\n",
    "\n",
    "Un **SQL Agent** pu√≤ interrogare database usando linguaggio naturale.\n",
    "\n",
    "**Database Chinook**: Database di esempio che simula un negozio di musica digitale con:\n",
    "- Artisti, Album, Brani\n",
    "- Clienti, Fatture, Ordini\n",
    "- Dipendenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica che il database esista\n",
    "import os\n",
    "\n",
    "db_path = \"../data/resources/Chinook.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"‚úÖ Database trovato: {db_path}\")\n",
    "    print(f\"   Dimensione: {os.path.getsize(db_path) / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(f\"‚ùå Database non trovato: {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connettiamoci al database\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{db_path}\")\n",
    "\n",
    "print(\"‚úÖ Connesso al database\\n\")\n",
    "print(\"üìä Tabelle disponibili:\")\n",
    "print(db.get_usable_table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esploriamo la struttura del database\n",
    "print(\"üîç Schema della tabella 'Artist':\\n\")\n",
    "print(db.get_table_info([\"Artist\"]))\n",
    "\n",
    "print(\"\\nüîç Schema della tabella 'Album':\\n\")\n",
    "print(db.get_table_info([\"Album\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8946d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query SQL diretta\n",
    "result = db.run(\"SELECT * FROM Artist LIMIT 5\")\n",
    "print(\"üìù Primi 5 artisti:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25465c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo tools per SQL\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=model)\n",
    "sql_tools = toolkit.get_tools()\n",
    "\n",
    "print(f\"‚úÖ {len(sql_tools)} SQL tools creati:\\n\")\n",
    "for tool in sql_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un SQL Agent con la nuova API\n",
    "sql_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=sql_tools,\n",
    "    system_prompt=\"\"\"Sei un esperto analista di database.\n",
    "\n",
    "Quando l'utente fa una domanda sui dati:\n",
    "1. Esamina lo schema delle tabelle rilevanti\n",
    "2. Costruisci la query SQL appropriata\n",
    "3. Esegui la query\n",
    "4. Interpreta i risultati in modo chiaro\n",
    "\n",
    "‚ö†Ô∏è IMPORTANTE:\n",
    "- Usa LIMIT per query esplorative\n",
    "- Controlla sempre i risultati prima di fare operazioni DML\n",
    "- Se non sei sicuro, chiedi conferma\n",
    "\n",
    "Rispondi sempre in italiano.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SQL Agent creato con la nuova API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SQL Agent\n",
    "response = sql_agent.invoke({\n",
    "    \"messages\": \"Quali sono i 5 artisti con pi√π album nel database?\"\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Risposta SQL Agent:\\n\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altra query di esempio\n",
    "response = sql_agent.invoke({\n",
    "    \"messages\": \"Qual √® il totale delle vendite per paese?\"\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Risposta SQL Agent:\\n\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae385ccd",
   "metadata": {},
   "source": [
    "## 4. üìö RAG: Retrieval Augmented Generation\n",
    "\n",
    "**RAG** combina:\n",
    "1. **Retrieval**: Cerca documenti rilevanti da una knowledge base\n",
    "2. **Augmented**: Arricchisce il prompt con informazioni recuperate\n",
    "3. **Generation**: LLM genera la risposta basandosi sui documenti\n",
    "\n",
    "**Vantaggi:**\n",
    "- Risposte basate su documenti specifici\n",
    "- Riduce allucinazioni\n",
    "- Permette di usare informazioni non presenti nel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4002e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica PDF\n",
    "pdf_path = \"../data/resources/acmecorp-employee-handbook.pdf\"\n",
    "\n",
    "if os.path.exists(pdf_path):\n",
    "    print(f\"‚úÖ PDF trovato: {pdf_path}\")\n",
    "    print(f\"   Dimensione: {os.path.getsize(pdf_path) / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(f\"‚ùå PDF non trovato: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d9982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installiamo le dipendenze per RAG\n",
    "# uv pip install pypdf faiss-cpu langchain-community\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# opzionale \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"‚úÖ Librerie RAG importate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ea2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carica il PDF\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ PDF caricato\")\n",
    "print(f\"   Numero di pagine: {len(documents)}\")\n",
    "print(f\"   Esempio contenuto prima pagina:\\n\")\n",
    "print(documents[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dividi il documento in chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Caratteri per chunk\n",
    "    chunk_overlap=200,  # Sovrapposizione tra chunks\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Documento diviso in chunks\")\n",
    "print(f\"   Numero di chunks: {len(chunks)}\")\n",
    "print(f\"   Esempio chunk:\\n\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86425abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crea embeddings e vector store\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "print(\"üîÑ Creazione vector store in corso (pu√≤ richiedere alcuni secondi)...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"‚úÖ Vector store FAISS creato!\")\n",
    "print(f\"   Numero di vettori: {vectorstore.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1622f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Test di ricerca semantica\n",
    "query = \"Qual √® la politica delle ferie?\"\n",
    "relevant_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"üîç Top 3 documenti rilevanti per: '{query}'\\n\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. Pagina {doc.metadata.get('page', 'N/A')}:\")\n",
    "    print(f\"   {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6793814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Creiamo un Retriever Tool\n",
    "from langchain.tools import tool\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Top 3 risultati\n",
    ")\n",
    "\n",
    "@tool\n",
    "def employee_handbook_search(query: str) -> str:\n",
    "    \"\"\"Cerca informazioni nel manuale dei dipendenti di ACME Corp.\n",
    "        \n",
    "    Usa questo tool per rispondere a domande su:\n",
    "    - Politiche aziendali\n",
    "    - Benefit e ferie\n",
    "    - Codice di condotta\n",
    "    - Procedure HR\n",
    "\n",
    "    Input: una domanda in linguaggio naturale\"\"\"\n",
    "    try:\n",
    "        docs = retriever.invoke(query)\n",
    "        if not docs:\n",
    "            return \"Nessuna informazione trovata nel manuale.\"\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        return f\"Informazioni trovate nel manuale:\\n\\n{context}\"\n",
    "    except Exception as e:\n",
    "        return f\"Errore durante la ricerca: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Retriever tool creato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Creiamo un RAG Agent con la nuova API\n",
    "rag_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[employee_handbook_search],\n",
    "    system_prompt=\"\"\"Sei un assistente HR di ACME Corp specializzato nel manuale dei dipendenti.\n",
    "\n",
    "Quando rispondi a domande:\n",
    "1. Usa il tool di ricerca per trovare informazioni rilevanti nel manuale\n",
    "2. Basa la tua risposta SOLO sulle informazioni trovate\n",
    "3. Se le informazioni non sono nel manuale, dillo chiaramente\n",
    "4. Cita sempre la fonte (pagina) delle informazioni\n",
    "\n",
    "‚ö†Ô∏è IMPORTANTE: Non inventare informazioni. Se non sai qualcosa, ammettilo.\n",
    "\n",
    "Rispondi sempre in italiano in modo professionale ma amichevole.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Agent creato con la nuova API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG Agent\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": \"Quanti giorni di ferie ho diritto?\"\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Risposta RAG Agent:\\n\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altra domanda\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": \"Qual √® la politica aziendale sul lavoro remoto?\"\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Risposta RAG Agent:\\n\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52138c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test con domanda fuori dal manuale\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": \"Qual √® lo stipendio medio in azienda?\"\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Risposta RAG Agent:\\n\")\n",
    "print(response[\"messages\"][-1].content)\n",
    "print(\"\\nüí° Nota: L'agente dovrebbe dire che questa informazione non √® nel manuale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53712fa",
   "metadata": {},
   "source": [
    "## üíæ Salvataggio del Vector Store\n",
    "\n",
    "Per evitare di ricreare gli embeddings ogni volta, salviamo il vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva vector store su disco\n",
    "vectorstore.save_local(\"../data/faiss_index\")\n",
    "print(\"‚úÖ Vector store salvato in '../data/faiss_index'\")\n",
    "\n",
    "# Per ricaricare in futuro:\n",
    "# vectorstore = FAISS.load_local(\n",
    "#     \"../data/faiss_index\",\n",
    "#     embeddings,\n",
    "#     allow_dangerous_deserialization=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd47d3",
   "metadata": {},
   "source": [
    "## üéØ Esercizio Finale: Agente Multi-Tool\n",
    "\n",
    "Combina tutti i tools in un unico agente super-potente!\n",
    "\n",
    "**Challenge**: Crea un agente che pu√≤:\n",
    "1. Cercare informazioni online (Tavily)\n",
    "2. Interrogare il database Chinook\n",
    "3. Rispondere su politiche aziendali (RAG)\n",
    "\n",
    "**Esempio di interazione:**\n",
    "- \"Cerca online i migliori album del 2025, poi dimmi quali di questi artisti sono nel nostro database\"\n",
    "- \"Qual √® la nostra politica ferie e quanti clienti abbiamo nel database?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ca7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il tuo codice qui!\n",
    "\n",
    "# Suggerimento: combina i tools\n",
    "# all_tools = [search, retriever_tool] + sql_tools\n",
    "\n",
    "# super_agent = create_agent(\n",
    "#     model=model,\n",
    "#     tools=all_tools,\n",
    "#     system_prompt=\"...\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360857aa",
   "metadata": {},
   "source": [
    "## üìö Riepilogo\n",
    "\n",
    "In questa lezione hai imparato:\n",
    "\n",
    "- ‚úÖ **Ricerca Web**: Integrare Tavily per informazioni aggiornate\n",
    "- ‚úÖ **Summarization**: Usare `SummarizationMiddleware` ufficiale per conversazioni lunghe\n",
    "- ‚úÖ **Human-in-the-Loop**: Implementare con `interrupt()` di LangGraph\n",
    "- ‚úÖ **SQL Agent**: Interrogare database con linguaggio naturale\n",
    "- ‚úÖ **RAG**: Rispondere basandosi su documenti specifici con FAISS\n",
    "\n",
    "### üîë Concetti Chiave\n",
    "\n",
    "1. **Tavily > Google** per ricerche ottimizzate AI\n",
    "2. **Nuova API `create_agent`** da `langchain.agents` (non pi√π `langgraph.prebuilt`)\n",
    "3. **SummarizationMiddleware** ufficiale con trigger multipli (messages, tokens, fraction)\n",
    "4. **`interrupt()`** nativo per Human-in-the-Loop invece di decorator custom\n",
    "5. **SQL Agents** democratizzano l'accesso ai dati\n",
    "6. **RAG** riduce allucinazioni e permette knowledge base custom\n",
    "7. **Vector stores** (FAISS) rendono la ricerca semantica efficiente\n",
    "\n",
    "### üÜï Novit√† API LangChain v1\n",
    "\n",
    "**Cosa √® Cambiato:**\n",
    "\n",
    "| Vecchia API | Nuova API | Vantaggi |\n",
    "\n",
    "|------------|-----------|----------|- üîó [Migration Guide](https://docs.langchain.com/oss/python/langchain/migration)\n",
    "\n",
    "| `langgraph.prebuilt.create_react_agent` | `langchain.agents.create_agent` | Pi√π semplice, unified interface |- üîó [LangGraph Interrupts](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/)\n",
    "\n",
    "| Middleware custom | `langchain.agents.middleware.*` | Standardizzati, testati, documentati |- üîó [SummarizationMiddleware](https://docs.langchain.com/oss/python/langchain/agents/middleware/summarization)\n",
    "\n",
    "| Decorator custom | `interrupt()` da `langgraph.types` | Nativo, supporto checkpoint |- üîó [LangChain Agents Docs](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "\n",
    "\n",
    "\n",
    "**Parametri `create_agent`:**### üìù Riferimenti\n",
    "\n",
    "- `model`: Modello LLM (string o istanza)\n",
    "\n",
    "- `tools`: Lista di tools- Gestione errori e retry avanzati\n",
    "\n",
    "- `system_prompt`: Prompt di sistema (nuovo!)- Monitoring e observability\n",
    "\n",
    "- `middleware`: Lista di middleware (nuovo!)- Deploy in produzione\n",
    "\n",
    "- `checkpointer`: Per persistenza- Workflow orchestration con LangGraph\n",
    "\n",
    "- `store`: Per storage cross-thread- Agenti multi-step complessi\n",
    "\n",
    "- `interrupt_before/after`: Per human-in-the-loopNella prossima lezione:\n",
    "\n",
    "\n",
    "### üöÄ Prossimi Passi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-corso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
