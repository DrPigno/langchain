{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8e8484",
   "metadata": {},
   "source": [
    "## 1. Setup Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5652da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfe9ef",
   "metadata": {},
   "source": [
    "## 2. MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691bbc4",
   "metadata": {},
   "source": [
    "### Server locali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccaf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import asyncio\n",
    "\n",
    "# # Fix for Windows issues in Jupyter notebooks\n",
    "# if sys.platform == \"win32\":\n",
    "#     # 1. Use ProactorEventLoop for subprocess support\n",
    "#     if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "#         asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "#     # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "#     if \"ipykernel\" in sys.modules:\n",
    "#         sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98034329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# client = MultiServerMCPClient(\n",
    "#     {\n",
    "#         \"local_server\": {\n",
    "#                 \"transport\": \"stdio\",\n",
    "#                 \"command\": \"python\",\n",
    "#                 \"args\": [\"../data/resources/2.1_mcp_server.py\"],\n",
    "#             }\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f35dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get tools\n",
    "# tools = await client.get_tools()\n",
    "\n",
    "# # get resources\n",
    "# resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# # get prompts\n",
    "# prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "# prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import create_agent\n",
    "# from langchain_cerebras import ChatCerebras\n",
    "\n",
    "# model=ChatCerebras(model=\"gpt-oss-120b\")\n",
    "\n",
    "# agent = create_agent(\n",
    "#     model=model,\n",
    "#     tools=tools,\n",
    "#     system_prompt=prompt\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.messages import HumanMessage\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# response = await agent.ainvoke(\n",
    "#     {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "#     config=config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d834a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in response['messages']:\n",
    "#     r.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf1923",
   "metadata": {},
   "source": [
    "### Server Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "    \"kiwi-com-flight-search\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://mcp.kiwi.com\"\n",
    "    }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "model=ChatCerebras(model=\"gpt-oss-120b\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    system_prompt=\"You are a travel assistant. Help users find flights based on their requests. No follow up questions. Answer in the user's language. Maximum 200 words. Keep the response of the tool under 500 words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad312ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "config = {\"configurable\": {\"thread_id\": \"flight_search_1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Trovami un volo da New York a Parigi il 25 marzo 2026\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in response['messages']:\n",
    "    r.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e171f4",
   "metadata": {},
   "source": [
    "## 3. Wrapper dinamici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de720710",
   "metadata": {},
   "source": [
    "### Modelli dinamici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed08b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from typing import Callable\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "large_model = ChatCerebras(model=\"gpt-oss-120b\")\n",
    "standard_model = ChatCerebras(model=\"llama-3.3-70b\")\n",
    "\n",
    "@wrap_model_call\n",
    "def state_based_model(request: ModelRequest, \n",
    "handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
    "    \"\"\"Select model based on State conversation length.\"\"\"\n",
    "    # request.messages is a shortcut for request.state[\"messages\"]\n",
    "    message_count = len(request.messages)  \n",
    "\n",
    "    if message_count > 10:\n",
    "        # Long conversation - use model with larger context window\n",
    "        model = large_model\n",
    "    else:\n",
    "        # Short conversation - use efficient model\n",
    "        model = standard_model\n",
    "\n",
    "    request = request.override(model=model)  \n",
    "\n",
    "    return handler(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=standard_model,\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"You are roleplaying a real life helpful office intern.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65753916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\"),\n",
    "        AIMessage(content=\"Yes, I gave it a light watering this morning.\"),\n",
    "        HumanMessage(content=\"Has it grown much this week?\"),\n",
    "        AIMessage(content=\"It's sprouted two new leaves since Monday.\"),\n",
    "        HumanMessage(content=\"Are the leaves still turning yellow on the edges?\"),\n",
    "        AIMessage(content=\"A little, but it's looking healthier overall.\"),\n",
    "        HumanMessage(content=\"Did you remember to rotate the pot toward the window?\"),\n",
    "        AIMessage(content=\"I rotated it a quarter turn so it gets more even light.\"),\n",
    "        HumanMessage(content=\"How often should we be fertilizing this plant?\"),\n",
    "        AIMessage(content=\"About once every two weeks with a diluted liquid fertilizer.\"),\n",
    "        HumanMessage(content=\"When should we expect to have to replace the pot?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54183a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b46eae",
   "metadata": {},
   "source": [
    "### Prompt dinamici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de267417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dataclass\n",
    "class LanguageContext:\n",
    "    user_language: str = \"English\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_language_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_language = request.runtime.context.user_language\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_language != \"English\":\n",
    "        return f\"{base_prompt} only respond in {user_language}.\"\n",
    "    elif user_language == \"English\":\n",
    "        return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "model = ChatCerebras(model=\"gpt-oss-120b\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    context_schema=LanguageContext,\n",
    "    middleware=[user_language_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15276a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hello, how are you?\")]},\n",
    "    context=LanguageContext(user_language=\"Irish\")\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hello, how are you?\")]},\n",
    "    context=LanguageContext(user_language=\"Spanish\")\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4aa784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hello, how are you?\")]},\n",
    "    context=LanguageContext(user_language=\"French\")\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a76feb",
   "metadata": {},
   "source": [
    "### Tools dinamici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158627d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Dict, Any\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_tavily import TavilySearch  # updated at 1.0\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../data/resources/Chinook.db\")\n",
    "\n",
    "tavily_search = TavilySearch(max_results=3)\n",
    "\n",
    "data = tavily_search.invoke({\"query\": \"What is LangGraph?\"})\n",
    "search_docs = data.get(\"results\", data)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_search.invoke({\"query\": query})\n",
    "\n",
    "@tool\n",
    "def sql_query(query: str) -> str:\n",
    "\n",
    "    \"\"\"Obtain information from the database using SQL queries\"\"\"\n",
    "\n",
    "    try:\n",
    "        return db.run(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class UserRole:\n",
    "    user_role: str = \"external\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from typing import Callable\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_tool_call(request: ModelRequest, \n",
    "handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
    "\n",
    "    \"\"\"Dynamically call tools based on the runtime context\"\"\"\n",
    "\n",
    "    user_role = request.runtime.context.user_role\n",
    "    \n",
    "    if user_role == \"internal\":\n",
    "        pass # internal users get access to all tools\n",
    "    else:\n",
    "        tools = [web_search] # external users only get access to web search\n",
    "        request = request.override(tools=tools) \n",
    "\n",
    "    return handler(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcdcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "model = ChatCerebras(model=\"gpt-oss-120b\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[web_search, sql_query],\n",
    "    middleware=[dynamic_tool_call],\n",
    "    context_schema=UserRole\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How many artists are in the database?\")]},\n",
    "    context={\"user_role\": \"external\"}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e44699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How many artists are in the database?\")]},\n",
    "    context={\"user_role\": \"internal\"}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075cc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
